{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78563c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tracking with mlflow\n",
    "import os\n",
    "from mlflow.models import infer_signature\n",
    "import pandas as pd\n",
    "#from urlib.parse import urlparse\n",
    "import mlflow\n",
    "#from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "import numpy as np\n",
    "import mlflow.sklearn\n",
    "import joblib\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecee520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline \n",
    "\n",
    "# defining numerical and categorical columns\n",
    "\n",
    "# preprocessing pipeline the numerical features that is all the features in the dataset\n",
    "# defining pipeline\n",
    "\n",
    "# pipeline for  the log transformation to handle skew features\n",
    "log_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n",
    "    StandardScaler())\n",
    "\n",
    "\n",
    "\n",
    "# building the preprocessing pipeline to apply the transformations such as\n",
    "# imputation of missing values using median values, scaling and log transformation to the numerical features\n",
    "preprocessing = make_column_transformer(\n",
    "    (log_pipeline, make_column_selector(dtype_include=np.number)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1249a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../artifacts/ml_training_data.csv\")\n",
    "data.columns, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552731f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"../artifacts/ml_test_data.csv\")\n",
    "test_data.columns, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa54a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data.drop(columns=[\"calories\"])\n",
    "y_test = test_data.calories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53e6d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the preprocessing pipeline\n",
    "processor = preprocessing.fit(X_train)\n",
    "X_train_processed = processor.transform(X_train)\n",
    "processor.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55e2b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model pipeline - from our training the best model was random forest regressor\n",
    "# check the default parameters of the random forest regressor\n",
    "rf = make_pipeline(\n",
    "    preprocessing, RandomForestRegressor(random_state=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb8598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking cross validation scores\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf_rmses = -cross_val_score(\n",
    "    rf, X_train, y_train, scoring=\"neg_mean_absolute_error\", cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60290b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(rf_rmses).describe() # check the mean and std of the cross validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ca979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the performance of a simple linear regression model as a baseline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lgr = make_pipeline(\n",
    "    preprocessing, LinearRegression()\n",
    ")\n",
    "lgr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc77263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model\n",
    "x_train_preds = lgr.predict(X_train)\n",
    "mean_squared_error(y_train, x_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f5e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_train, x_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795eb210",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_train, x_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b79db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining hyparameter grid search space to get the best performance for the random forest regressor\n",
    "\n",
    "param_grid = {\n",
    "    'randomforestregressor__n_estimators': [100, 200, 500],       # Number of trees in the forest\n",
    "    'randomforestregressor__max_features': ['sqrt', 'log2', 0.5], # Number of features to consider at each split\n",
    "    'randomforestregressor__max_depth': [10, 20, 30, None],       # Maximum depth of the tree\n",
    "    'randomforestregressor__min_samples_split': [2, 5, 10],       # Minimum number of samples required to split\n",
    "    'randomforestregressor__min_samples_leaf': [1, 2, 4],         # Minimum number of samples required at a leaf node\n",
    "    'randomforestregressor__bootstrap': [True, False],            # Whether bootstrap samples are used\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "#grid_search.fit(X_train, y_train) : takes time to train so i switch to random search to save computation time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946aa9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint\n",
    "# Using RandomizedSearchCV for hyperparameter tuning\n",
    "\n",
    "param_distributions = {\n",
    "    'randomforestregressor__n_estimators': randint(100, 600),\n",
    "    'randomforestregressor__max_features': ['sqrt', 'log2', 0.5],\n",
    "    'randomforestregressor__max_depth': [10, 20, 30, None],\n",
    "    'randomforestregressor__min_samples_split': randint(2, 11),\n",
    "    'randomforestregressor__min_samples_leaf': randint(1, 5),\n",
    "    'randomforestregressor__bootstrap': [True, False],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=25,              # try 25 random combinations\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best R² score:\", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe28608",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Predict on train set\n",
    "y_pred = best_model.predict(X_train)\n",
    "\n",
    "# Compute metrics\n",
    "r2 = r2_score(y_train, y_pred)\n",
    "mae = mean_absolute_error(y_train, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35d374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute metrics\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb3cce9",
   "metadata": {},
   "source": [
    "Comparing the performance of the baseline model and the tuned random forest regressor model, we can see a significant improvement in all metrics. The R² score has increased, while both MAE and RMSE have decreased, indicating that the hyperparameter tuning has effectively enhanced the model's predictive capabilities.\n",
    "\n",
    "* Also the model performance is not overfitting the dataset as it perform reasonable well on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6091131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can skip this code\n",
    "\n",
    "#mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\"))\n",
    "mlflow.set_experiment(\"RandomForest_Regression_Pipeline\")\n",
    "\n",
    "\n",
    "\n",
    "# ===  The trained model ===\n",
    "best_model = random_search.best_estimator_  # or grid_search.best_estimator_\n",
    "best_params = random_search.best_params_     # or grid_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# === Save locally ===\n",
    "local_model_path = \"../models/best_random_forest_pipeline.joblib\"\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "joblib.dump(best_model, local_model_path)\n",
    "print(f\" Model saved locally at: {local_model_path}\")\n",
    "\n",
    "# === Log to MLflow on DagsHub ===\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\"))\n",
    "with mlflow.start_run():\n",
    "    signature=infer_signature(X_train,y_train)\n",
    "    # Log best hyperparameters\n",
    "    mlflow.log_params(best_params)\n",
    "    \n",
    "    input_example = X_train.iloc[[0]]\n",
    "    \n",
    "    # Log performance metrics\n",
    "    mlflow.log_metric(\"r2_score\", best_score)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=best_model,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=\"RandomForest_Regression_Pipeline_v1\",\n",
    "        input_example=input_example\n",
    "    )\n",
    "\n",
    "print(\" Model and params logged successfully to DagsHub MLflow!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16167594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71588679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a21235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd4d51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
